{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import nltk\n",
      "import stop_words\n",
      "import gensim"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html\n",
      "\n",
      "https://ariddell.org/lda.html"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Upload the Documents\n",
      "docs = []\n",
      "for filename in os.listdir('/home/jerry/Data/Hopper_Project/1506_demacro_abstracts/'):\n",
      "    f = open('/home/jerry/Data/Hopper_Project/1506_demacro_abstracts/' + filename)\n",
      "    docs.append(f.read())\n",
      "    f.close()\n",
      "print( len(docs) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "8295\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Cleaning the Documents\n",
      "\n",
      "# Tokenization \n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "tokenizer = RegexpTokenizer(r'\\w+')\n",
      "\n",
      "# Stop Words\n",
      "from stop_words import get_stop_words\n",
      "en_stop = get_stop_words('en')\n",
      "\n",
      "# Stemming\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "p_stemmer = PorterStemmer()\n",
      "\n",
      "# Constructing a docment-term matrix\n",
      "from gensim import corpora, models"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Loop through all abstracts\n",
      "texts = []\n",
      "\n",
      "for i in docs:\n",
      "    \n",
      "    # clean and tokenize document string\n",
      "    raw = i.lower()\n",
      "    raw = unicode(raw, errors = 'ignore') #bc I was getting errors\n",
      "                                          #something not unicode\n",
      "    tokens = tokenizer.tokenize(raw)\n",
      "    \n",
      "    # remove stop words from tokens\n",
      "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
      "    \n",
      "    # stem tokens\n",
      "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
      "    \n",
      "    # add tokens to list\n",
      "    texts.append(stemmed_tokens)\n",
      "\n",
      "# turn tokenized documents into id <-> term dictionary    \n",
      "dictionary = corpora.Dictionary(texts)\n",
      "\n",
      "# covnert tokenized documents into a document-term matrix\n",
      "corpus = [dictionary.doc2bow(text) for text in texts]\n",
      "\n",
      "# generate LDA model\n",
      "ldamodel = gensim.models.ldamodel.LdaModel(corpus,\n",
      "                                           num_topics = 5, \n",
      "                                           id2word = dictionary,\n",
      "                                           passes = 20)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# generate LDA model, more topics\n",
      "ldamodel_10 = gensim.models.ldamodel.LdaModel(corpus,\n",
      "                                           num_topics = 10, \n",
      "                                           id2word = dictionary,\n",
      "                                           passes = 20)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(ldamodel_10.print_topics(num_topics=10, num_words=3))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'0.029*group + 0.020*algebra + 0.011*gener', u'0.020*theori + 0.018*graph + 0.015*field', u'0.013*galaxi + 0.012*observ + 0.010*ray', u'0.011*spin + 0.010*state + 0.010*field', u'0.020*model + 0.012*energi + 0.010*decay', u'0.015*equat + 0.014*system + 0.013*solut', u'0.013*model + 0.012*use + 0.011*algorithm', u'0.041*n + 0.038*1 + 0.028*2', u'0.027*0 + 0.017*star + 0.015*2', u'0.021*function + 0.011*result + 0.011*space']\n"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So I've done some preliminary fitting for the topic model. 5 topics, 20 passes. The categories make sense (as in I can kind of understand their groupings), but I want to do some experimentation for how I can better fit the model. I really do like this idea of finding the 'latent' topics, because maybe the subject categories (math, phys, cs, etc) aren't the best way to group the papers. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dictionary[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'dictionary' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-1-1e961f917705>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m: name 'dictionary' is not defined"
       ]
      }
     ],
     "prompt_number": 1
    }
   ],
   "metadata": {}
  }
 ]
}