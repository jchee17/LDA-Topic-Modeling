{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Workbook to explore LDA Topic Modeling for 1506_demacro dataset (for now just the abstracts). \n",
      "\n",
      "http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/\n",
      "\n",
      "**Introduction to Latent Dirichlet Allocation**\n",
      "\n",
      "- LDA represnts documents as mixtures of topics, spit out words with certain probabilities\n",
      "- *Generative Model. Assumes when documents produced:\n",
      "    1. (N) number of words the document will have.\n",
      "    2. Choose a topic mixture for the document according to a Dirchlet distribution over a fixed set of K topics.\n",
      "    3. Generate each word $w_i$ in the document by \n",
      "        * Picking a topic\n",
      "        * Using the topic to pick a word, given the probabilties of word occurences in the topic\n",
      "- Learning. Choose some K fixed number of topics to discover. \n",
      "    1. Gibbs Sampling. \n",
      "        * Go through each document, randomly assign each word to one of of K topics. Random assignment gives topic representations of all documents and word distributions of all topics (not very good though).\n",
      "        * For each word w $\\in$ d...\n",
      "            - For each topic t compute\n",
      "                1. p(topic t | document d) = proportion of words in document d that are currently assigned to topic t\n",
      "                2. p(word w | topic t) = proportion of assignments to topic t over all documents that come from this word w\n",
      "            - Reassign w a new topic with probability p(topic t| document d) * p(word w | topic t) the two previously caluclated probabilities. According to our generative model, essentially the probability that topic t generated word w. \n",
      "            - In other words we're assuming all topic assignments except for the current word are correct, and update the assignment of the current word using our model of how documents are generated. \n",
      "         * After repeating a large number of times, will reach a roughly steady state where the assignments are pretty good. Use these assignments to estimate the topic mixtures of each document. \n",
      "- Intuitive / Layman's Explanation\n",
      "    1. See Edwin Chen's blog post.\n",
      "\n",
      "               "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Another Look into Latent Dirichlet Allocation**\n",
      "\n",
      "https://www.quora.com/What-is-a-good-explanation-of-Latent-Dirichlet-Allocation\n",
      "\n",
      "Naomi P Saphra's Post.\n",
      "\n",
      "- LDA common method of topic modeling\n",
      "- To LDA, a document is just a collection of topics where each topic has a particular probability of generating a particular word. From say a potential sports article the word \"average\" appears 4 times. What's probability of a sports topic generating that many instances of \"average\"?. \n",
      "- Determine by looking at each training document as a \"bag of words\" pulled from a distribution selected by a Dirichlet process.\n",
      "\n",
      "- Dirichlet Distribution\n",
      "- 2 Dirichlet distributions, one over all topics and one for each topic, over all words in the said topic. \n",
      "\n",
      "- LDA called \"Latent\" because topics are latent variables, only directly observing the words. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Another Another Look into LDA**\n",
      "\n",
      "https://www.quora.com/What-is-a-good-explanation-of-Latent-Dirichlet-Allocation\n",
      "\n",
      "Ira Cohen's Post. \n",
      "\n",
      "Experimented with LDA to group IT events. Words were type of events, and documents were time windows. Vector representation was whether an event of type i occured during a particular time window. Topics were groups of events that tended to occur together at some frequency. \n",
      "\n",
      "Link to paper: http://link.springer.com/chapter/10.1007/978-3-642-04180-8_32\n",
      "(downloaded as well)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now I look at the 1506_demacro abstract data\n",
      "\n",
      "import numpy as np\n",
      "import lda\n",
      "import lda.datasets\n",
      "\n",
      "X = lda.datasets.load_reuters()\n",
      "vocab = lda.datasets.load_reuters_vocab()\n",
      "titles = lda.datasets.load_reuters_titles()\n",
      "\n",
      "X.shape\n",
      "\n",
      "model = lda.LDA(n_topics=20, n_iter=500, random_state=1)\n",
      "model.fit(X)\n",
      "topic_word = model.topic_word_\n",
      "n_top_words = 8\n",
      "for i, topic_dist in enumerate(topic_word):\n",
      "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-n_top_words:-1]\n",
      "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Topic 0: government british minister west group letters party\n",
        "Topic 1: church first during people political country ceremony\n",
        "Topic 2: elvis king wright fans presley concert life\n",
        "Topic 3: yeltsin russian russia president kremlin michael romania\n",
        "Topic 4: pope vatican paul surgery pontiff john hospital\n",
        "Topic 5: family police miami versace cunanan funeral home\n",
        "Topic 6: south simpson born york white north african\n",
        "Topic 7: order church mother successor since election religious\n",
        "Topic 8: charles prince diana royal queen king parker\n",
        "Topic 9: film france french against actor paris bardot\n",
        "Topic 10: germany german war nazi christian letter book\n",
        "Topic 11: east prize peace timor quebec belo indonesia\n",
        "Topic 12: n't told life people church show very\n",
        "Topic 13: years world time year last say three\n",
        "Topic 14: mother teresa heart charity calcutta missionaries sister\n",
        "Topic 15: city salonika exhibition buddhist byzantine vietnam swiss\n",
        "Topic 16: music first people tour including off opera\n",
        "Topic 17: church catholic bernardin cardinal bishop death cancer\n",
        "Topic 18: harriman clinton u.s churchill paris president ambassador\n",
        "Topic 19: century art million museum city churches works\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "doc_topic = model.doc_topic_\n",
      "for i in range(10):\n",
      "    print(\"{} (top topic: {})\".format(titles[i], doc_topic[i].argmax()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 UK: Prince Charles spearheads British royal revolution. LONDON 1996-08-20 (top topic: 8)\n",
        "1 GERMANY: Historic Dresden church rising from WW2 ashes. DRESDEN, Germany 1996-08-21 (top topic: 1)\n",
        "2 INDIA: Mother Teresa's condition said still unstable. CALCUTTA 1996-08-23 (top topic: 14)\n",
        "3 UK: Palace warns British weekly over Charles pictures. LONDON 1996-08-25 (top topic: 8)\n",
        "4 INDIA: Mother Teresa, slightly stronger, blesses nuns. CALCUTTA 1996-08-25 (top topic: 14)\n",
        "5 INDIA: Mother Teresa's condition unchanged, thousands pray. CALCUTTA 1996-08-25 (top topic: 14)\n",
        "6 INDIA: Mother Teresa shows signs of strength, blesses nuns. CALCUTTA 1996-08-26 (top topic: 14)\n",
        "7 INDIA: Mother Teresa's condition improves, many pray. CALCUTTA, India 1996-08-25 (top topic: 14)\n",
        "8 INDIA: Mother Teresa improves, nuns pray for \"miracle\". CALCUTTA 1996-08-26 (top topic: 14)\n",
        "9 UK: Charles under fire over prospect of Queen Camilla. LONDON 1996-08-26 (top topic: 8)\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(X[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[1 0 1 ..., 0 0 0]\n",
        " [7 0 2 ..., 0 0 0]\n",
        " [0 0 0 ..., 0 0 0]\n",
        " ..., \n",
        " [1 0 1 ..., 0 0 0]\n",
        " [1 0 1 ..., 0 0 0]\n",
        " [1 0 1 ..., 0 0 0]]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1 0 1 ..., 0 0 0]\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}